<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Foundation-model-based Action selection for Behavior Trees in Navigation</title>
  <style>
    * {
      box-sizing: border-box;
    }

    body {
      font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #333;
    }

    header {
      background: #2c3e50;
      color: white;
      padding: 2rem 1rem;
      text-align: center;
    }

    header h1 {
      margin: 0 0 0.5rem;
    }

    .container {
      width: 100%;
      max-width: 100%;
      padding: 2rem 10%;
      background: #f9f9f9;
    }

    section {
      margin-bottom: 2rem;
    }

    h2 {
      border-bottom: 2px solid #eee;
      padding-bottom: 0.5rem;
      margin-top: 0;
    }

    a {
      color: #2980b9;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .authors, .links {
      margin-bottom: 1rem;
    }

    .paper-img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 1rem auto;
      border: 1px solid #ccc;
      border-radius: 6px;
    }

    video {
      width: 100%;
      height: auto;
      border-radius: 6px;
    }

    .video-container {
      position: relative;
      width: 100%;
      padding-bottom: 56.25%;
      height: 0;
      overflow: hidden;
      border-radius: 6px;
    }

    .video-container video,
    .video-container iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

    .prompt-text {
      background: #fff;
      padding: 1rem;
      border: 1px solid #ddd;
      border-radius: 6px;
      margin-top: 1rem;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #777;
      padding: 2rem 1rem;
      background-color: #fff;
      border-top: 1px solid #eee;
    }

    pre {
      background-color: #f4f4f4;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 6px;
    }
  </style>
  <script>
    function showPrompt() {
      const selected = document.getElementById("prompt-select").value;
      document.getElementById("zs").style.display = selected === "zs" ? "block" : "none";
      document.getElementById("icl").style.display = selected === "icl" ? "block" : "none";
      document.getElementById("cot").style.display = selected === "cot" ? "block" : "none";
      document.getElementById("icl+cot").style.display = selected === "icl+cot" ? "block" : "none";
    }
  </script>
</head>
<body>
  <header>
    <h1>Foundation-model-based Action selection for Behavior Trees in Navigation</h1>
    <p>Conference Name, 2025</p>
  </header>

  <div class="container">
    <section class="authors">
      <h2>Authors</h2>
      <p>Michele Moriconi<sup>1</sup>, Stefan Laible<sup>2</sup>, Carmine Recchiuto<sup>1</sup></p>
      <p><sup>1</sup>Universit√† degli Studi di Genova &nbsp; <sup>2</sup>Bosch Research</p>
    </section>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
        Autonomous navigation in dynamic environments remains a central challenge in robotics, particularly within industrial settings populated by humans, vehicles, and 
        other robots. Traditional navigation pipelines often lack semantic understanding, treating obstacles as simple geometric shapes and failing to adapt effectively to 
        unforeseen scenarios. This work introduces a novel architecture that enhances robotic obstacle avoidance by integrating Foundation Models (FMs) with Behavior Trees 
        (BTs), the standard decision-making framework in ROS2. The proposed pipeline features a Perception Module that extracts semantic information from RGB-D inputs using 
        vision-language models, a Reasoning Module that interprets textual obstacle descriptions via large language models to select context-aware actions, and a BT-based 
        execution system that orchestrates behavior accordingly. Experimental validation using a custom warehouse-like dataset and a range of prompting techniques 
        demonstrates that the inclusion of FMs significantly improves semantic interpretation and decision-making capabilities, without compromising the 
        safety and reliability of traditional methods.
      </p>
    </section>

    <section class="figure">
      <h2>Overview</h2>
      <img class="paper-img" src="Media/system_overview.jpg" alt="Overview figure or architecture diagram">
      <p>Figure: A brief caption of the main figure or architecture diagram.</p>
    </section>

    <section class="prompts">
      <h2>Prompts</h2>
      <p>
        In this section, we present the prompts used in our experiments. The prompts are designed to guide the Foundation Model in selecting the 
        appropriate action for the robot based on the detected obstacles. The prompts are structured using the Ollama modelfile format.
        Specifically, SYSTEM identifies the system prompt that sets the context for the model, while MESSAGE user and MESSAGE assistant
        represent the user and assistant messages, respectively, and are used to provide examples of the expected input and output. The 
        prompts are categorized into four types: Zero-shot, In-context Learning (ICL), Chain-of-Thought (CoT), and ICL + CoT. Select the 
        prompt type from the dropdown menu to view the corresponding prompt.
      </p>
      <select id="prompt-select" onchange="showPrompt()" style="padding: 0.5rem 1rem; font-size: 1rem; border: 2px solid #2980b9; border-radius: 6px; background-color: #f9f9f9; color: #333; cursor: pointer; transition: all 0.3s ease;">
        <option value="zs">Zero-shot</option>
        <option value="icl">In-context Learning</option>
        <option value="cot">Chain-of-Thought</option>
        <option value="icl+cot">ICL + CoT</option>
      </select>
      <div id="zs" class="prompt-text">
        <p>SYSTEM</p>
        <p><i>
          You are a helpful assistant that helps an autonomous vehicle navigate an industrial environment. The robot is 
          currently navigating towards a goal, when it detects one or more obstacles in front of itself. The robot needs to 
          reach its goal in the shortest time possible. The input is a list of obstacles. Your goal is to choose the correct 
          action the robot should take. The set of possible actions is: SPEAK, AVOID_FAST, AVOID_SLOW, REPORT_EMERGENCY, and 
          WAIT. The SPEAK action makes the robot speak and should be used when the obstacle is a person or a group of people 
          that are walking or standing. The AVOID_FAST action makes the robot navigate around the obstacle at cruising speed 
          and should be used when the obstacle is an inanimate object or a unused vehicle. The AVOID_SLOW action makes the 
          robot navigate around the obstacle at reduced speed and should be used when the obstacle is a person or a group of 
          people that are sitting or operating machinery. The REPORT_EMERGENCY action should be used when the obstacle is an 
          injured person or something that is an hazard. The WAIT action makes the robot wait and should be used only when the 
          obstacle is a human operated vehicle or an autonomous robot. The output must be a JSON file and nothing else. The 
          'answer' field must be a string and can only be SPEAK, AVOID_FAST, AVOID_SLOW, REPORT_EMERGENCY, or WAIT.
        </i></p>
      </div>

      <div id="icl" class="prompt-text" style="display: none;">
        <p>SYSTEM</p>
        <p><i>
          You are a helpful assistant that helps an autonomous vehicle navigate an industrial environment. The robot is 
          currently navigating towards a goal, when it detects one or more obstacles in front of itself. The robot needs to 
          reach its goal in the shortest time possible. The input is a list of obstacles. Your goal is to choose the correct 
          action the robot should take. The set of possible actions is: SPEAK, AVOID_FAST, AVOID_SLOW, REPORT_EMERGENCY, and 
          WAIT. The SPEAK action makes the robot speak and should be used when the obstacle is a person or a group of people 
          that are walking or standing. The AVOID_FAST action makes the robot navigate around the obstacle at cruising speed 
          and should be used when the obstacle is an inanimate object or a unused vehicle. The AVOID_SLOW action makes the 
          robot navigate around the obstacle at reduced speed and should be used when the obstacle is a person or a group of 
          people that are sitting or operating machinery. The REPORT_EMERGENCY action should be used when the obstacle is an 
          injured person or something that is an hazard. The WAIT action makes the robot wait and should be used only when the 
          obstacle is a human operated vehicle or an autonomous robot. The output must be a JSON file and nothing else. The 
          'answer' field must be a string and can only be SPEAK, AVOID_FAST, AVOID_SLOW, REPORT_EMERGENCY, or WAIT.
        </i></p>
        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Box", "obstacle2": "Unused vehicle"</i></p>
        <p>MESSAGE assistant</p>
        <p><i>"answer": "AVOID_FAST"</i></p>

        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Chemical spill"</i></p>
        <p>MESSAGE assistant</p>
        "answer": "REPORT_EMERGENCY"

        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Worker repairing equipment"</i></p>
        <p>MESSAGE assistant</p>
        <p><i>"answer": "AVOID_SLOW"</i></p>

        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Man walking while carrying a box", "obstacle2": "Group of people chatting"</i></p>
        <p>MESSAGE assistant</p>
        <p><i>"answer": "SPEAK"</i></p>

        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Forklift carrying materials" </i></p>
        <p>MESSAGE assistant</p>
        <p><i>"answer": "WAIT"</i></p>
      </div>

      <div id="cot" class="prompt-text" style="display: none;">
        <p>SYSTEM</p>
        <p><i>
          You are a helpful assistant that helps an autonomous vehicle navigate an industrial environment. The robot is currently navigating towards 
          a goal, when it detects one or more obstacles in front of itself. The robot needs to reach its goal in the shortest time possible. The 
          input is a list of obstacles. Your goal is to choose the correct action the robot should take. The set of possible actions is: SPEAK, 
          AVOID_FAST, AVOID_SLOW, REPORT_EMERGENCY, and WAIT. The SPEAK action makes the robot speak and should be used when the obstacle is a person 
          or a group of people that are walking or standing. The AVOID_FAST action makes the robot navigate around the obstacle at cruising speed and 
          should be used when the obstacle is an inanimate object or a unused vehicle. The AVOID_SLOW action makes the robot navigate around the 
          obstacle at reduced speed and should be used when the obstacle is a person or a group of people that are unlikely to move soon. The 
          REPORT_EMERGENCY action should be used when the obstacle is an injured person or something that is an hazard. The WAIT action makes the 
          robot wait and should be used only when the obstacle is a human operated vehicle or an autonomous robot. The output must be a JSON file. The 
          first output field is 'reasoning' and it is a step-by-step explanation about which action should be chosen. The second output field is 
          'answer' and it is the final answer, which has to be consistent with the reasoning. The 'answer' field must be a string and can only be 
          SPEAK, AVOID_FAST, AVOID_SLOW, REPORT_EMERGENCY, or WAIT.
        </i></p>
      </div>

      <div id="icl+cot" class="prompt-text" style="display: none;">
        <p>SYSTEM</p>
        <p><i>
          You are a helpful assistant that helps an autonomous vehicle navigate an industrial environment. The robot is currently navigating towards 
          a goal, when it detects one or more obstacles in front of itself. The robot needs to reach its goal in the shortest time possible. The 
          input is a list of obstacles. Your goal is to choose the correct action the robot should take. The set of possible actions is: SPEAK, 
          AVOID_FAST, AVOID_SLOW, REPORT_EMERGENCY, and WAIT. The SPEAK action makes the robot speak and should be used when the obstacle is a person 
          or a group of people that are walking or standing. The AVOID_FAST action makes the robot navigate around the obstacle at cruising speed and 
          should be used when the obstacle is an inanimate object or a unused vehicle. The AVOID_SLOW action makes the robot navigate around the 
          obstacle at reduced speed and should be used when the obstacle is a person or a group of people that are unlikely to move soon. The 
          REPORT_EMERGENCY action should be used when the obstacle is an injured person or something that is an hazard. The WAIT action makes the 
          robot wait and should be used only when the obstacle is a human operated vehicle or an autonomous robot. The output must be a JSON file. The 
          first output field is 'reasoning' and it is a step-by-step explanation about which action should be chosen. The second output field is 
          'answer' and it is the final answer, which has to be consistent with the reasoning. The 'answer' field must be a string and can only be 
          SPEAK, AVOID_FAST, AVOID_SLOW, REPORT_EMERGENCY, or WAIT.
        </i></p>
        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Box"</i></p>
        <p>MESSAGE assistant</p>
        <p><i> "reasoning": ["The obstacle is an inanimate object (box)",
            "There are no people so AVOID_SLOW and SPEAK can be ruled out",
            "There are no vehicles or autonomous robots so WAIT can be ruled out",
            "The obstacle (box) is not an hazard so REPORT_EMERGENCY can be ruled out",
            "The correct answer is AVOID_FAST"
          ],
          "answer": "AVOID_FAST"
        </i></p>

        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Chemical spill"</i></p>
        <p>MESSAGE assistant</p>
        <p><i>
          "reasoning": [
            "The obstacle is an inanimate object (chemical spill)",
            "There are no people so AVOID_SLOW and SPEAK can be ruled out",
            "There are no vehicles or autonomous robots so WAIT can be ruled out",
            "The obstacle (chemical spill) is an hazard so REPORT_EMERGENCY is the correct answer"
          ],
          "answer": "REPORT_EMERGENCY"
        </i></p>

        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Worker repairing equipment"</i></p>
        <p>MESSAGE assistant</p>
        <p><i>
          "reasoning": [
            "The obstacle is a person (worker repairing equipment)",
            "There is a person so AVOID_FAST can be ruled out",
            "There are no vehicles or autonomous robots so WAIT can be ruled out",
            "The obstacle (worker repairing equipment) is not an hazard so REPORT_EMERGENCY can be ruled out",
            "The person is working so he probably will not move so the correct answer is AVOID_SLOW"
          ],
          "answer": "AVOID_SLOW"
        </i></p>

        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Man walking while carrying a box"</i></p>
        <p>MESSAGE assistant</p>
        <p><i>
          "reasoning": [
            "The first obstacle is a person (Man walking while carrying a box)",
            "There is a person so AVOID_FAST can be ruled out",
            "There are no vehicles or autonomous robots so WAIT can be ruled out",
            "The obstacles are not a hazard so REPORT_EMERGENCY can be ruled out",
            "There is a person walking so SPEAK is the correct answer"
          ],
          "answer": "SPEAK"
        </i></p>

        <p>MESSAGE user</p>
        <p><i>"obstacle1": "Forklift carrying materials"</i></p>
        <p>MESSAGE assistant</p>
        <p><i>
          "reasoning": [
            "There is one obstacle",
            "The obstacle is a vehicle (forklift carrying materials)",
            "There is a vehicle so AVOID_FAST can be ruled out",
            "There is a vehicle so WAIT is the correct answer"
          ],
          "answer": "WAIT"
        </i></p>
    </section>

    <section class="behavior-tree">
      <h2>Behavior Tree</h2>
      This section provides a detailed view of the Behavior Tree (BT) used in our architecture. The BT is designed to manage the robot's actions based on the
      detected obstacles and the selected actions. The BT is structured to ensure that the robot can adapt its behavior dynamically based on the environment and the
      detected obstacles. The BT is implemented in ROS2 and is designed to be modular and extensible, allowing for easy integration of new behaviors and actions as needed.
      <img class="paper-img" src="Media/full_tree.png" alt="Behavior Tree">
      <p>Figure: The Behavior Tree of the proposed architecture.</p>

    <section class="video">
      <h2>Video</h2>
      <p>
        This video demonstrates the execution of the proposed pipeline, with one example for each of the five different actions. It is important to keep in mind that the 
        models are running on an edge device and it communicates with the robot through ROS2. The communication between the robot and the edge device has not been 
        optimized yet, so the latency is not representative of the final performance.
      </p>
      <video controls loop autoplay muted>
        <source src="Media/media1.mp4" type="video/mp4">
        Your browser does not support the video tag.
        <a href="Media/media1.mp4" download>Download the video</a>
      </video>
      <p>Video: The executions of the proposed pipeline, showcasing the five different actions.</p>
    </section>

    <!-- <section class="links">
      <h2>Resources</h2>
      <ul>
        <li><a href="link-to-paper.pdf" target="_blank">üìÑ Paper (PDF)</a></li>
        <li><a href="https://github.com/yourusername/yourproject" target="_blank">üíª Code</a></li>
      </ul>
    </section> -->

    <!-- <section class="bibtex">
      <h2>BibTeX</h2>
      <pre>
@inproceedings{moriconi2025fabnav,
  title={Foundation-model-based Action selection for Behavior Trees in Navigation},
  author={Moriconi, Michele and Laible, Stefan and Recchiuto, Carmine},
  booktitle={Proceedings of the Conference Name},
  year={2025}
}
      </pre>
    </section> -->
  </div>

  <footer>
    Last updated: April 2025.
  </footer>
</body>
</html>